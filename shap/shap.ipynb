{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y pandas\n",
        "!pip install pandas --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        },
        "id": "g-OisDEUQQH3",
        "outputId": "78bea9ec-cdd0-424f-e6e5-912a1ce64be1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: pandas 2.3.3\n",
            "Uninstalling pandas-2.3.3:\n",
            "  Successfully uninstalled pandas-2.3.3\n",
            "Collecting pandas\n",
            "  Using cached pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Using cached pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.4 MB)\n",
            "Installing collected packages: pandas\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.3 which is incompatible.\n",
            "cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\n",
            "dask-cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pandas-2.3.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pandas"
                ]
              },
              "id": "a1d4ca30d6694ca58fb2661e5b1307f8"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sys\n",
        "os.kill(os.getpid(), 9)\n"
      ],
      "metadata": {
        "id": "i0fgcI8dQkqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# üß© STEP 0: ÂÆâË£Ö‰æùËµñÔºà‰ªÖÁ¨¨‰∏ÄÊ¨°ËøêË°åÔºâ\n",
        "# ==========================================\n",
        "!pip install shap transformers datasets torch pandas tqdm --quiet\n"
      ],
      "metadata": {
        "id": "mzCcrbxXP9G6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import shap\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "print(\"‚úÖ pandas version:\", pd.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXMcdW74QnqS",
        "outputId": "96479dd7-5795-4e1c-964a-8372bb6fbba5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ pandas version: 2.3.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import shap\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# ‚úÖ 1Ô∏è‚É£ Âä†ËΩΩÊ®°ÂûãÔºàbinary toxic classifierÔºâ\n",
        "model_name = \"SkolkovoInstitute/roberta_toxicity_classifier\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "model.eval()\n",
        "\n",
        "# ‚úÖ 2Ô∏è‚É£ ÂÆö‰πâÈ¢ÑÊµãÂáΩÊï∞\n",
        "def predict_toxic(texts):\n",
        "    if isinstance(texts, str):\n",
        "        texts = [texts]\n",
        "    elif isinstance(texts, (tuple, np.ndarray)):\n",
        "        texts = list(texts)\n",
        "    elif not isinstance(texts, list):\n",
        "        try:\n",
        "            texts = list(texts)\n",
        "        except TypeError:\n",
        "            raise ValueError(\"Unsupported input type for predict_toxic\")\n",
        "\n",
        "    texts = [str(text) for text in texts]\n",
        "\n",
        "    enc = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=256)\n",
        "    with torch.no_grad():\n",
        "        logits = model(**enc).logits\n",
        "    # Return class-1 logit (more linear, larger-magnitude signal for SHAP)\n",
        "    return logits[:, 1].unsqueeze(1).cpu().numpy()\n",
        "\n",
        "# ‚úÖ 3Ô∏è‚É£ ÂÆö‰πâ SHAP masker & explainer\n",
        "# By default the HuggingFace tokenizer produces subword tokens (BPE). If you want\n",
        "# to run SHAP at word-level (one word == one token, split by whitespace) we need\n",
        "# to provide a small wrapper that mimics the minimal tokenizer interface SHAP\n",
        "# expects (callable returning a dict with 'input_ids' and a convert_tokens_to_string method).\n",
        "\n",
        "# Set to True to use word-level tokens (split on whitespace). False to use model tokenizer (subword).\n",
        "USE_WORD_LEVEL = True\n",
        "# Set to True to run a deterministic leave-one-out (L1O) per-token impact instead of SHAP\n",
        "# L1O guarantees every token occurrence is measured but is more expensive (one model call per token)\n",
        "USE_LEAVE_ONE_OUT = True\n",
        "\n",
        "class WordTokenizer:\n",
        "    \"\"\"A tiny tokenizer-like wrapper that splits text on whitespace and\n",
        "    provides a minimal HF-like interface required by shap.maskers.Text.\n",
        "    This lets SHAP treat each whitespace-delimited word as a single token.\n",
        "    \"\"\"\n",
        "    def __call__(self, text):\n",
        "        if not isinstance(text, str):\n",
        "            text = str(text)\n",
        "        tokens = text.split()\n",
        "        # SHAP inspects tokenizer(\"\")['input_ids'] to get prefix/suffix info;\n",
        "        # return tokens as 'input_ids' (strings) ‚Äî this is enough for the Text masker.\n",
        "        return {\"input_ids\": tokens}\n",
        "\n",
        "    def convert_tokens_to_string(self, tokens):\n",
        "        return \" \".join(tokens)\n",
        "\n",
        "    def decode(self, ids):\n",
        "        return \" \".join(ids)\n",
        "\n",
        "\n",
        "if USE_WORD_LEVEL:\n",
        "    masker = shap.maskers.Text(WordTokenizer())\n",
        "else:\n",
        "    masker = shap.maskers.Text(tokenizer)\n",
        "\n",
        "explainer = shap.Explainer(predict_toxic, masker)\n",
        "\n",
        "# ‚úÖ 4Ô∏è‚É£ Ëé∑ÂèñÊâÄÊúâ subreddit CSV Êñá‰ª∂Ë∑ØÂæÑ\n",
        "csv_files = [\n",
        "    \"final_version.csv\"\n",
        "]\n",
        "\n",
        "# ‚úÖ 5Ô∏è‚É£ Âæ™ÁéØÂ§ÑÁêÜÊØè‰∏™Êñá‰ª∂\n",
        "for file_path in csv_files:\n",
        "    subreddit_name = os.path.splitext(os.path.basename(file_path))[0]\n",
        "    print(f\"\\nüöÄ Processing {subreddit_name} ...\")\n",
        "\n",
        "    df = pd.read_csv(file_path)\n",
        "    df[\"body\"] = df[\"body_nopunct_apostrophe\"].fillna(\"\")\n",
        "    # prepare texts and original ids (use 'id' or 'Sentence_ID' if present, else index)\n",
        "    max_rows = 300  #### Ë∞ÉÊï¥sentenceÊï∞Èáè target == 300\n",
        "    texts_to_explain = df[\"body_nopunct_apostrophe\"].tolist()[:max_rows]\n",
        "    if 'id' in df.columns:\n",
        "        orig_ids = df['id'].tolist()[:max_rows]\n",
        "    elif 'Sentence_ID' in df.columns:\n",
        "        orig_ids = df['Sentence_ID'].tolist()[:max_rows]\n",
        "    else:\n",
        "        orig_ids = (df.index + 1).tolist()[:max_rows]\n",
        "\n",
        "    print(f\"üß© Explaining {len(texts_to_explain)} comments from {subreddit_name}...\")\n",
        "\n",
        "    rows = []\n",
        "\n",
        "    if USE_LEAVE_ONE_OUT:\n",
        "        # Deterministic leave-one-out: for each sentence and each word occurrence, remove the word\n",
        "        # and measure change in model logit (orig_logit - masked_logit)\n",
        "        print(\"‚öôÔ∏è Running leave-one-out per-token (this may be slow)...\")\n",
        "        for i, text in enumerate(texts_to_explain):\n",
        "            sent_id = orig_ids[i]\n",
        "            # split on whitespace to get words (preserves apostrophes if present)\n",
        "            tokens = [t for t in str(text).split() if t.strip()]\n",
        "            if not tokens:\n",
        "                continue\n",
        "            # original logit\n",
        "            orig_logit = float(predict_toxic([text])[0])\n",
        "            for pos in range(len(tokens)):\n",
        "                masked_tokens = tokens[:pos] + tokens[pos+1:]\n",
        "                masked_text = \" \".join(masked_tokens)\n",
        "                masked_logit = float(predict_toxic([masked_text])[0])\n",
        "                impact = orig_logit - masked_logit\n",
        "                # clip to [-1, 1] for consistency with earlier output\n",
        "                impact_clipped = float(np.clip(impact, -1.0, 1.0))\n",
        "                rows.append({\n",
        "                    \"Sentence_ID\": sent_id,\n",
        "                    \"Sentence_Order\": i + 1,\n",
        "                    \"Text\": text,\n",
        "                    \"Token\": tokens[pos],\n",
        "                    \"Token_Position\": pos + 1,\n",
        "                    \"SHAP_Impact_on_Toxic\": impact_clipped,\n",
        "                    \"Interpretation\": \"‚Üë increase toxic\" if impact_clipped > 0 else \"‚Üì decrease toxic\"\n",
        "                })\n",
        "    else:\n",
        "        shap_values = explainer(texts_to_explain)\n",
        "        # For every sentence, record an entry for each token occurrence (with position)\n",
        "        for i, text in enumerate(texts_to_explain):\n",
        "            toks = np.array(shap_values.data[i])\n",
        "            vals = np.array(shap_values.values[i]).flatten()\n",
        "            sent_id = orig_ids[i]\n",
        "            for pos, (t, v) in enumerate(zip(toks, vals), start=1):\n",
        "                if isinstance(t, str) and t.strip() not in [\"[CLS]\", \"[SEP]\", \"[PAD]\", \"\"]:\n",
        "                    # Clip SHAP impact to [-1, 1]\n",
        "                    v_clipped = float(np.clip(v, -1.0, 1.0))\n",
        "                    rows.append({\n",
        "                        \"Sentence_ID\": sent_id,\n",
        "                        \"Sentence_Order\": i + 1,\n",
        "                        \"Text\": text,\n",
        "                        \"Token\": t,\n",
        "                        \"Token_Position\": pos,\n",
        "                        \"SHAP_Impact_on_Toxic\": v_clipped,\n",
        "                        \"Interpretation\": \"‚Üë increase toxic\" if v_clipped > 0 else \"‚Üì decrease toxic\"\n",
        "                    })\n",
        "\n",
        "    df_out = pd.DataFrame(rows)\n",
        "    df_out = df_out.sort_values([\"Sentence_ID\", \"SHAP_Impact_on_Toxic\"], ascending=[True, False])\n",
        "\n",
        "    out_name = f\"toxic_shap_table_{subreddit_name}.csv\"\n",
        "    df_out.to_csv(out_name, index=False, encoding=\"utf-8-sig\")\n",
        "    print(f\"üíæ Saved as {out_name}\")\n",
        "\n",
        "print(\"\\n‚úÖ All subreddit files processed successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6Rv0MqZxXOg",
        "outputId": "69e67f1c-5c7e-4a27-ebb2-0e9a5f2bcd70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at SkolkovoInstitute/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "/tmp/ipython-input-2804026245.py:111: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  orig_logit = float(predict_toxic([text])[0])\n",
            "/tmp/ipython-input-2804026245.py:115: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  masked_logit = float(predict_toxic([masked_text])[0])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üöÄ Processing final_version ...\n",
            "üß© Explaining 300 comments from final_version...\n",
            "‚öôÔ∏è Running leave-one-out per-token (this may be slow)...\n",
            "üíæ Saved as toxic_shap_table_final_version.csv\n",
            "\n",
            "‚úÖ All subreddit files processed successfully!\n"
          ]
        }
      ]
    }
  ]
}